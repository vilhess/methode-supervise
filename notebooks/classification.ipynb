{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importation de quelques librairies\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# importation des librairies pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chemin de nos données d'entrainement et de test\n",
    "\n",
    "train_path = '../data/data_cla/stars_train_new.csv' # replace with your path\n",
    "test_path = '../data/data_cla/stars_test_new.csv' # replace with your path\n",
    "\n",
    "# chargement des données\n",
    "\n",
    "df_train = pd.read_csv(train_path).drop('obj_ID', axis=1)\n",
    "df_test = pd.read_csv(test_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation des données\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# informations sur les données\n",
    "\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# description des données\n",
    "\n",
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forme des données\n",
    "\n",
    "print(df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disrtibution des classes\n",
    "\n",
    "df_train['label'].hist()\n",
    "plt.title('Star Class')\n",
    "plt.xlabel('Class')\n",
    "plt.xticks([0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de corrélation\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(df_train.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution des variables\n",
    "\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "for i in range(len(df_train.columns)):\n",
    "    fig.add_subplot(5, 3, i+1)\n",
    "    sns.histplot(df_train.iloc[:, i], color='green', label=df_train.columns[i])\n",
    "    # show the mean and median\n",
    "    plt.axvline(df_train.iloc[:, i].mean(), linestyle='dashed', color='red', label='mean')\n",
    "    plt.axvline(df_train.iloc[:, i].median(), linestyle='dashed', color='blue', label='median')\n",
    "    plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Séparation train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparation des données en train test\n",
    "\n",
    "X = df_train.drop('label', axis=1)\n",
    "y = df_train['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Définition de notre espaces de paramètres\n",
    "\n",
    "params = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5]\n",
    "}\n",
    "\n",
    "# Recherche des meilleurs paramètres\n",
    "grid = GridSearchCV(rf, params, cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"paramètres optimaux : {grid.best_params_}\")\n",
    "\n",
    "print(f\"meilleur score : {grid.best_score_}\")\n",
    "\n",
    "# Prédiction sur les données de test\n",
    "\n",
    "y_preds = grid.predict(X_test)\n",
    "\n",
    "# Matrice de confusion\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_test, y_preds), annot=True, cmap='coolwarm')\n",
    "\n",
    "# Sauvegarde des résultats pour streamlit\n",
    "\n",
    "new_results = pd.DataFrame({'y_test': y_test,\n",
    "                            'y_preds': y_preds})\n",
    "\n",
    "new_results.to_csv('../résultats_models/classif/random_forest.csv', index=False)\n",
    "\n",
    "# soumission des résultats\n",
    "\n",
    "rf = RandomForestClassifier(**grid.best_params_)\n",
    "\n",
    "rf.fit(df_train.drop('label', axis=1), df_train['label'])\n",
    "\n",
    "y_preds = rf.predict(df_test.drop('obj_ID', axis=1))\n",
    "\n",
    "submission = pd.DataFrame({'obj_ID': df_test['obj_ID'],\n",
    "                            'label': y_preds})\n",
    "\n",
    "submission.to_csv('../soumission/classif/random_forest.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Définition de notre modèle\n",
    "\n",
    "cat = CatBoostClassifier(n_estimators=3000, max_depth=10, learning_rate=0.01)\n",
    "\n",
    "# Entrainement du modèle\n",
    "\n",
    "cat.fit(X_train, y_train, verbose=1)\n",
    "\n",
    "# Prédiction sur les données de test\n",
    "\n",
    "y_preds = cat.predict(X_test)\n",
    "\n",
    "# Matrice de confusion\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_test, y_preds), annot=True, cmap='coolwarm')\n",
    "\n",
    "# Sauvegarde des résultats pour streamlit\n",
    "\n",
    "new_results = pd.DataFrame({'y_test': y_test,\n",
    "                            'y_preds': y_preds.ravel()})\n",
    "\n",
    "new_results.to_csv('../résultats_models/classif/catboost.csv', index=False)\n",
    "\n",
    "# soumission des résultats\n",
    "\n",
    "cat = CatBoostClassifier(n_estimators=3000, max_depth=10, learning_rate=0.01)\n",
    "\n",
    "cat.fit(df_train.drop('label', axis=1), df_train['label'], verbose=1)\n",
    "\n",
    "y_preds = cat.predict(df_test.drop('obj_ID', axis=1))\n",
    "\n",
    "submission = pd.DataFrame({'obj_ID': df_test['obj_ID'],\n",
    "                            'label': y_preds.ravel()})\n",
    "\n",
    "submission.to_csv('../soumission/classif/catboost.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = TensorDataset(torch.from_numpy(X_train.values).float(), torch.from_numpy(y_train.values).long())\n",
    "testset = TensorDataset(torch.from_numpy(X_test.values).float(), torch.from_numpy(y_test.values).long())\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=32, shuffle=False)\n",
    "\n",
    "fullset = TensorDataset(torch.from_numpy(df_train.drop('label', axis=1).values).float(), torch.from_numpy(df_train['label'].values).long())\n",
    "fullset_loder = DataLoader(fullset, batch_size=32, shuffle=True)\n",
    "full_testset = TensorDataset(torch.from_numpy(df_test.drop('obj_ID', axis=1).values).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNetClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "model = NeuralNetClassifier(input_size=8, hidden_size=5, num_classes=3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/15, loss=0.4296 accuracy = 74.55254703992657\n",
      "epoch 2/15, loss=0.5321 accuracy = 74.39192290041304\n",
      "epoch 3/15, loss=0.5732 accuracy = 75.47039926571821\n",
      "epoch 4/15, loss=0.4148 accuracy = 77.89505889551782\n",
      "epoch 5/15, loss=0.4128 accuracy = 86.87471317117944\n",
      "epoch 6/15, loss=0.4916 accuracy = 89.76594768242313\n",
      "epoch 7/15, loss=0.1054 accuracy = 91.00504818724185\n",
      "epoch 8/15, loss=0.3282 accuracy = 93.4220590484932\n",
      "epoch 9/15, loss=0.2149 accuracy = 93.55208811381368\n",
      "epoch 10/15, loss=0.2386 accuracy = 94.00336545816124\n",
      "epoch 11/15, loss=0.1587 accuracy = 93.90393146703381\n",
      "epoch 12/15, loss=0.0789 accuracy = 94.70705216460149\n",
      "epoch 13/15, loss=0.2625 accuracy = 94.85237876701851\n",
      "epoch 14/15, loss=0.1731 accuracy = 94.72234970169802\n",
      "epoch 15/15, loss=0.2286 accuracy = 95.12773443475601\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(15):\n",
    "    for i, (inputs, labels) in enumerate(trainloader):\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        for inputs, labels in testloader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            n_samples += labels.size(0)\n",
    "            n_correct += (predicted == labels).sum().item()\n",
    "        acc = 100.0 * n_correct / n_samples\n",
    "        print(f'epoch {epoch+1}/{15}, loss={loss.item():.4f} accuracy = {acc}')\n",
    "\n",
    "# Prédiction sur les données de test\n",
    "y_preds = []\n",
    "with torch.no_grad():\n",
    "    for inputs in testset:\n",
    "        outputs = model(inputs[0])\n",
    "        _, predicted = torch.max(outputs.data, 0)\n",
    "        y_preds.append(predicted.item())\n",
    "\n",
    "new_results = pd.DataFrame({'y_test': y_test,\n",
    "                            'y_preds': y_preds})\n",
    "\n",
    "new_results.to_csv('../résultats_models/classif/neural_net.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/15, loss=0.8666 accuracy = 72.29616031818877\n",
      "epoch 2/15, loss=0.9498 accuracy = 74.7590637907297\n",
      "epoch 3/15, loss=0.4636 accuracy = 78.65228698179592\n",
      "epoch 4/15, loss=0.3638 accuracy = 89.29172403243078\n",
      "epoch 5/15, loss=0.2998 accuracy = 88.66452501147316\n",
      "epoch 6/15, loss=0.1403 accuracy = 90.33960532354291\n",
      "epoch 7/15, loss=0.1673 accuracy = 89.15404619856203\n",
      "epoch 8/15, loss=0.3662 accuracy = 93.17729845494875\n",
      "epoch 9/15, loss=0.5687 accuracy = 90.91326296466269\n",
      "epoch 10/15, loss=0.0840 accuracy = 93.80449747590637\n",
      "epoch 11/15, loss=0.2790 accuracy = 94.27107235735046\n",
      "epoch 12/15, loss=0.3162 accuracy = 92.65718219366681\n",
      "epoch 13/15, loss=0.2782 accuracy = 93.94217530977512\n",
      "epoch 14/15, loss=0.2695 accuracy = 93.34557136301055\n",
      "epoch 15/15, loss=0.1629 accuracy = 92.74896741624599\n",
      "epoch 16/15, loss=0.4664 accuracy = 94.42404772831574\n",
      "epoch 17/15, loss=0.0621 accuracy = 94.24047728315742\n",
      "epoch 18/15, loss=0.7192 accuracy = 93.8427413186477\n",
      "epoch 19/15, loss=0.0985 accuracy = 94.50818418234664\n",
      "epoch 20/15, loss=0.0454 accuracy = 94.84472999847024\n",
      "epoch 21/15, loss=1.2236 accuracy = 94.85237876701851\n",
      "epoch 22/15, loss=0.0470 accuracy = 94.68410585895671\n",
      "epoch 23/15, loss=0.0101 accuracy = 94.86002753556677\n",
      "epoch 24/15, loss=0.0578 accuracy = 94.79883738718067\n",
      "epoch 25/15, loss=0.1072 accuracy = 95.32660241701086\n",
      "epoch 26/15, loss=0.0568 accuracy = 93.97277038396818\n",
      "epoch 27/15, loss=0.0757 accuracy = 94.10279944928867\n",
      "epoch 28/15, loss=0.0943 accuracy = 94.82943246137371\n",
      "epoch 29/15, loss=0.1604 accuracy = 91.57105705981337\n",
      "epoch 30/15, loss=0.0635 accuracy = 95.2348171944317\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetClassifier(input_size=8, hidden_size=5, num_classes=3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "model = NeuralNetClassifier(input_size=8, hidden_size=5, num_classes=3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "for epoch in range(30):\n",
    "    for i, (inputs, labels) in enumerate(fullset_loder):\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        for inputs, labels in testloader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            n_samples += labels.size(0)\n",
    "            n_correct += (predicted == labels).sum().item()\n",
    "        acc = 100.0 * n_correct / n_samples\n",
    "        print(f'epoch {epoch+1}/{15}, loss={loss.item():.4f} accuracy = {acc}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_preds = []\n",
    "    for inputs in full_testset:\n",
    "        outputs = model(inputs[0])\n",
    "        _, predicted = torch.max(outputs.data, 0)\n",
    "        y_preds.append(predicted.item())\n",
    "\n",
    "submission = pd.DataFrame({'obj_ID': df_test['obj_ID'],\n",
    "                            'label': y_preds})\n",
    "\n",
    "submission.to_csv('../soumission/classif/neural_net.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "df_train_scaled = scaler.fit_transform(df_train.drop('label', axis=1))\n",
    "df_test_scaled = scaler.transform(df_test.drop('obj_ID', axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Définition de notre espaces de paramètres\n",
    "\n",
    "params = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5]\n",
    "}\n",
    "\n",
    "# Recherche des meilleurs paramètres\n",
    "grid = GridSearchCV(rf, params, cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"paramètres optimaux : {grid.best_params_}\")\n",
    "\n",
    "print(f\"meilleur score : {grid.best_score_}\")\n",
    "\n",
    "# Prédiction sur les données de test\n",
    "\n",
    "y_preds = grid.predict(X_test_scaled)\n",
    "\n",
    "# Matrice de confusion\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_test, y_preds), annot=True, cmap='coolwarm')\n",
    "\n",
    "# Sauvegarde des résultats pour streamlit\n",
    "\n",
    "new_results = pd.DataFrame({'y_test': y_test,\n",
    "                            'y_preds': y_preds})\n",
    "\n",
    "new_results.to_csv('../résultats_models/classif/random_forest_scaled.csv', index=False)\n",
    "\n",
    "# soumission des résultats\n",
    "\n",
    "rf = RandomForestClassifier(**grid.best_params_)\n",
    "\n",
    "rf.fit(df_train_scaled.drop('label', axis=1), df_train_scaled['label'])\n",
    "\n",
    "y_preds = rf.predict(df_test_scaled.drop('obj_ID', axis=1))\n",
    "\n",
    "submission = pd.DataFrame({'obj_ID': df_test_scaled['obj_ID'],\n",
    "                            'label': y_preds})\n",
    "\n",
    "submission.to_csv('../soumission/classif/random_forest_scaled.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Définition de notre modèle\n",
    "\n",
    "cat = CatBoostClassifier(n_estimators=3000, max_depth=10, learning_rate=0.01)\n",
    "\n",
    "# Entrainement du modèle\n",
    "\n",
    "cat.fit(X_train_scaled, y_train, verbose=1)\n",
    "\n",
    "# Prédiction sur les données de test\n",
    "\n",
    "y_preds = cat.predict(X_test_scaled)\n",
    "\n",
    "# Matrice de confusion\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_test, y_preds), annot=True, cmap='coolwarm')\n",
    "\n",
    "# Sauvegarde des résultats pour streamlit\n",
    "\n",
    "new_results = pd.DataFrame({'y_test': y_test,\n",
    "                            'y_preds': y_preds.ravel()})\n",
    "\n",
    "new_results.to_csv('../résultats_models/classif/catboost_scaled.csv', index=False)\n",
    "\n",
    "# soumission des résultats\n",
    "\n",
    "cat = CatBoostClassifier(n_estimators=3000, max_depth=10, learning_rate=0.01)\n",
    "\n",
    "cat.fit(df_train_scaled.drop('label', axis=1), df_train_scaled['label'], verbose=1)\n",
    "\n",
    "y_preds = cat.predict(df_test_scaled.drop('obj_ID', axis=1))\n",
    "\n",
    "submission = pd.DataFrame({'obj_ID': df_test_scaled['obj_ID'],\n",
    "                            'label': y_preds.ravel()})\n",
    "\n",
    "submission.to_csv('../soumission/classif/catboost_scaled.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = TensorDataset(torch.from_numpy(X_train_scaled).float(), torch.from_numpy(y_train.values).long())\n",
    "testset = TensorDataset(torch.from_numpy(X_test_scaled).float(), torch.from_numpy(y_test.values).long())\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=32, shuffle=False)\n",
    "\n",
    "fullset = TensorDataset(torch.from_numpy(df_train_scaled).float(), torch.from_numpy(df_train['label'].values).long())\n",
    "fullset_loder = DataLoader(fullset, batch_size=32, shuffle=True)\n",
    "full_testset = TensorDataset(torch.from_numpy(df_test_scaled).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNetClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "model = NeuralNetClassifier(input_size=8, hidden_size=5, num_classes=3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/14, loss=0.2835 accuracy = 96.40507878231605\n",
      "epoch 2/14, loss=0.0610 accuracy = 96.15266942022335\n",
      "epoch 3/14, loss=0.0199 accuracy = 96.30564479118861\n",
      "epoch 4/14, loss=0.0802 accuracy = 96.2674009484473\n",
      "epoch 5/14, loss=0.0935 accuracy = 96.38213247667126\n",
      "epoch 6/14, loss=0.2010 accuracy = 96.34388863392994\n",
      "epoch 7/14, loss=0.2855 accuracy = 96.47391769925042\n",
      "epoch 8/14, loss=0.1567 accuracy = 96.44332262505736\n",
      "epoch 9/14, loss=0.1994 accuracy = 96.20621080006119\n",
      "epoch 10/14, loss=0.0296 accuracy = 96.31329355973688\n",
      "epoch 11/14, loss=0.1331 accuracy = 96.31329355973688\n",
      "epoch 12/14, loss=0.1300 accuracy = 96.48921523634695\n",
      "epoch 13/14, loss=0.0975 accuracy = 96.16796695731988\n",
      "epoch 14/14, loss=0.0357 accuracy = 96.48921523634695\n",
      "epoch 15/14, loss=0.1554 accuracy = 96.3285910968334\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(15):\n",
    "    for i, (inputs, labels) in enumerate(trainloader):\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        for inputs, labels in testloader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            n_samples += labels.size(0)\n",
    "            n_correct += (predicted == labels).sum().item()\n",
    "        acc = 100.0 * n_correct / n_samples\n",
    "        print(f'epoch {epoch+1}/{15}, loss={loss.item():.4f} accuracy = {acc}')\n",
    "\n",
    "# Prédiction sur les données de test\n",
    "y_preds = []\n",
    "with torch.no_grad():\n",
    "    for inputs in testset:\n",
    "        outputs = model(inputs[0])\n",
    "        _, predicted = torch.max(outputs.data, 0)\n",
    "        y_preds.append(predicted.item())\n",
    "\n",
    "new_results = pd.DataFrame({'y_test': y_test,\n",
    "                            'y_preds': y_preds})\n",
    "\n",
    "new_results.to_csv('../résultats_models/classif/neural_net_scaled.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/100, loss=0.8471 accuracy = 80.79394217530978\n",
      "epoch 2/100, loss=0.2325 accuracy = 91.90760287593697\n",
      "epoch 3/100, loss=0.1077 accuracy = 93.38381520575187\n",
      "epoch 4/100, loss=0.2807 accuracy = 94.18693590331956\n",
      "epoch 5/100, loss=0.2731 accuracy = 94.50053541379837\n",
      "epoch 6/100, loss=1.1107 accuracy = 95.02065167508032\n",
      "epoch 7/100, loss=0.0281 accuracy = 95.42603640813829\n",
      "epoch 8/100, loss=0.6470 accuracy = 95.4872265565244\n",
      "epoch 9/100, loss=0.3604 accuracy = 95.45663148233135\n",
      "epoch 10/100, loss=0.2084 accuracy = 95.61725562184488\n",
      "epoch 11/100, loss=1.6747 accuracy = 95.60960685329663\n",
      "epoch 12/100, loss=0.0129 accuracy = 95.51782163071745\n",
      "epoch 13/100, loss=0.0660 accuracy = 95.57901177910357\n",
      "epoch 14/100, loss=0.0382 accuracy = 95.78552852990668\n",
      "epoch 15/100, loss=0.0585 accuracy = 95.5713630105553\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetClassifier(input_size=8, hidden_size=5, num_classes=3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "model = NeuralNetClassifier(input_size=8, hidden_size=5, num_classes=3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "for epoch in range(15):\n",
    "    for i, (inputs, labels) in enumerate(fullset_loder):\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        for inputs, labels in testloader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            n_samples += labels.size(0)\n",
    "            n_correct += (predicted == labels).sum().item()\n",
    "        acc = 100.0 * n_correct / n_samples\n",
    "        print(f'epoch {epoch+1}/{15}, loss={loss.item():.4f} accuracy = {acc}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_preds = []\n",
    "    for inputs in full_testset:\n",
    "        outputs = model(inputs[0])\n",
    "        _, predicted = torch.max(outputs.data, 0)\n",
    "        y_preds.append(predicted.item())\n",
    "\n",
    "submission = pd.DataFrame({'obj_ID': df_test['obj_ID'],\n",
    "                            'label': y_preds})\n",
    "\n",
    "submission.to_csv('../soumission/classif/neural_net_scaled.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3 (main, Apr 19 2023, 18:49:55) [Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "970b6e94afd843568256675086d87407db15a8cd9814ea6ad86d69153853824c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
